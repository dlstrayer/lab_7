
@article{vanderheyden_is_2018,
	title = {Is More Screening Better? The Relationship Between Frequent Screening, Accurate Decisions, and Reading Proficiency},
	volume = {47},
	issn = {2372-966X},
	url = {https://www.tandfonline.com/doi/full/10.17105/SPR-2017-0017.V47-1},
	doi = {10.17105/SPR-2017-0017.V47-1},
	shorttitle = {Is More Screening Better?},
	abstract = {Screening is necessary to detect risk and prevent reading failure. Yet the amount of screening that commonly occurs in U.S. schools may undermine its value, creating more error in decision making and lost instructional opportunity. This 2-year longitudinal study examined the decision accuracy associated with collecting concurrent reading screening measures and in collecting fall and winter screening measures for all students in Grade 3 in 4 schools in a midwestern district (N = 428) in Year 1. This study also categorized children by proficiency on the Year 1 test and by the amount of screening they received in Year 1 in 7 schools in the district (N = 656) and then examined performance on the Year 2 test to examine screening benefit. Analyses included multiple regression, classification agreement, and multilevel modeling. Results found no added accuracy in using more than one screening in the fall and no added accuracy in using both fall and winter screening data. Students experienced on average a 1.57-point gain on the year-end test for each screening to which they were exposed. Follow-up analyses found benefit for the most at-risk students and little benefit of screening for proficient students.},
	pages = {62--82},
	number = {1},
	journaltitle = {School Psychology Review},
	shortjournal = {School Psychology Review},
	author = {{VanDerHeyden}, Amanda M. and Burns, Matthew K. and Bonifay, Wesley},
	editor = {Eckert, Tanya},
	urldate = {2022-03-20},
	date = {2018-03-01},
	langid = {english},
	file = {VanDerHeyden et al. - 2018 - Is More Screening Better The Relationship Between.pdf:files/201/VanDerHeyden et al. - 2018 - Is More Screening Better The Relationship Between.pdf:application/pdf},
}

@article{stockard_effectiveness_2018,
	title = {The Effectiveness of Direct Instruction Curricula: A Meta-Analysis of a Half Century of Research},
	volume = {88},
	issn = {0034-6543, 1935-1046},
	url = {http://journals.sagepub.com/doi/10.3102/0034654317751919},
	doi = {10.3102/0034654317751919},
	shorttitle = {The Effectiveness of Direct Instruction Curricula},
	abstract = {Quantitative mixed models were used to examine literature published from 1966 through 2016 on the effectiveness of Direct Instruction. Analyses were based on 328 studies involving 413 study designs and almost 4,000 effects. Results are reported for the total set and subareas regarding reading, math, language, spelling, and multiple or other academic subjects; ability measures; affective outcomes; teacher and parent views; and single-subject designs. All of the estimated effects were positive and all were statistically significant except results from metaregressions involving affective outcomes. Characteristics of the publications, methodology, and sample were not systematically related to effect estimates. Effects showed little decline during maintenance, and effects for academic subjects were greater when students had more exposure to the programs. Estimated effects were educationally significant, moderate to large when using the traditional psychological benchmarks, and similar in magnitude to effect sizes that reflect performance gaps between more and less advantaged students.},
	pages = {479--507},
	number = {4},
	journaltitle = {Review of Educational Research},
	shortjournal = {Review of Educational Research},
	author = {Stockard, Jean and Wood, Timothy W. and Coughlin, Cristy and Rasplica Khoury, Caitlin},
	urldate = {2022-03-20},
	date = {2018-08},
	langid = {english},
	file = {Stockard et al. - 2018 - The Effectiveness of Direct Instruction Curricula.pdf:files/206/Stockard et al. - 2018 - The Effectiveness of Direct Instruction Curricula.pdf:application/pdf},
}

@article{polikoff_instructional_2010,
	title = {Instructional Sensitivity as a Psychometric Property of Assessments: Winter, 2010},
	volume = {29},
	issn = {07311745},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1745-3992.2010.00189.x},
	doi = {10.1111/j.1745-3992.2010.00189.x},
	shorttitle = {Instructional Sensitivity as a Psychometric Property of Assessments},
	abstract = {Standards-based reform, as codiﬁed by the No Child Left Behind Act, relies on the ability of assessments to accurately reﬂect the learning that takes place in U.S. classrooms. However, this property of assessments—their instructional sensitivity—is rarely, if ever, investigated by test developers, states, or researchers. In this paper, the literature on the psychometric property of instructional sensitivity is reviewed. Three categories of instructional sensitivity measures are identiﬁed—those relying on item or test scores only, those relying on item or test scores and teacher reports of instruction, and strictly judgmental methods. Each method identiﬁed in the literature is discussed alongside the evidence for its utility. Finally, recommendations are made as to the proper role of instructional sensitivity in the evaluation of assessments used under standards-based reform.},
	pages = {3--14},
	number = {4},
	journaltitle = {Educational Measurement: Issues and Practice},
	author = {Polikoff, Morgan S.},
	urldate = {2022-03-20},
	date = {2010-12},
	langid = {english},
	file = {Polikoff - 2010 - Instructional Sensitivity as a Psychometric Proper.pdf:files/207/Polikoff - 2010 - Instructional Sensitivity as a Psychometric Proper.pdf:application/pdf},
}

@article{naumann_sensitivity_2019,
	title = {Sensitivity of test items to teaching quality},
	volume = {60},
	issn = {09594752},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0959475217307065},
	doi = {10.1016/j.learninstruc.2018.11.002},
	abstract = {Instructional sensitivity is the psychometric capacity of tests or single items of capturing eﬀects of classroom instruction. Yet, current item sensitivity measures’ relationship to (a) actual instruction and (b) overall test sensitivity is rather unclear. The present study aims at closing these gaps by investigating test and item sensitivity to teaching quality, reanalyzing data from a quasi-experimental intervention study in primary school science education (1026 students, 53 classes, Mage = 8.79 years, {SDage} = 0.49, 50\% female). We examine (a) the correlation of item sensitivity measures and the potential for cognitive activation in class and (b) consequences for test score interpretation when assembling tests from items varying in their degree of sensitivity to cognitive activation. Our study (a) provides validity evidence that item sensitivity measures may be related to actual classroom instruction and (b) points out that inferences on teaching drawn from test scores may vary due to test composition.},
	pages = {41--53},
	journaltitle = {Learning and Instruction},
	shortjournal = {Learning and Instruction},
	author = {Naumann, Alexander and Rieser, Svenja and Musow, Stephanie and Hochweber, Jan and Hartig, Johannes},
	urldate = {2022-03-20},
	date = {2019-04},
	langid = {english},
	file = {Naumann et al. - 2019 - Sensitivity of test items to teaching quality.pdf:files/209/Naumann et al. - 2019 - Sensitivity of test items to teaching quality.pdf:application/pdf},
}
